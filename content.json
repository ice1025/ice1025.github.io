[{"title":"elasticSearch  Query DSL(Filters)","date":"2017-05-05T08:53:53.303Z","path":"2017/05/05/elasticsearch_queryDSL/","text":"在一篇中我们实现了mysql数据实时同步至elasticSearch，接下来就是利用java API来实现数据查询，使用Filters方式查询注释写的很清楚，包含分页，关键字查询，范围查询等等 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101@RequestMapping(value = \"/testList/&#123;page&#125;\") public List&lt;Test&gt; list(HttpServletRequest request,@PathVariable(\"page\") Integer pageNo) throws ParseException &#123; int pageSize = 5; TransportClient client = EsUtil.getTransportClient(); SearchRequestBuilder srb = client.prepareSearch().setIndices(\"test\").setTypes(\"testInfo\"); /** * 有两种查询方式，query查询和filter查询 * 两者的区别： * query查询会计算一个分值，es中索引的数据都会存储一个_score分值，分值越高就代表越匹配 * filter查询不会去计算任何分值，也不关心返回的排序问题，因此效率会高一点。 * 经常使用过滤器，ES会自动的缓存过滤器的内容，这对于查询来说，会提高很多性能。 */ BoolFilterBuilder boolFilter = FilterBuilders.boolFilter(); /** * 通配符查询 查询name中带有 \"小\" 的 * * 匹配任意字符串 ? 匹配任意一个字符 */ boolFilter.must(FilterBuilders.queryFilter(QueryBuilders.wildcardQuery(\"name\",\"*小*\"))); /** * name 查询没有字段或没有值或值为null的数据 */ boolFilter.must(FilterBuilders.missingFilter(\"name\")); /** * 查询 name 不为null的数据 */ boolFilter.must(FilterBuilders.existsFilter(\"name\")); /** * idsFilter 查询 testInfo 中 id 为1 和 2 的， * \"testInfo\"为可变参数 */ boolFilter.must(FilterBuilders.idsFilter(\"testInfo\").addIds(\"1\",\"2\")); /** * 查询age等于22 或者 33 */ boolFilter.must(FilterBuilders.inFilter(\"age\",\"20\",\"33\")); /** * 查询name 等于 \"小明\" 的 */ boolFilter.must(FilterBuilders.termFilter(\"name\",\"小明\")); /** * 查询 createDate 在某个区间内 */ String date1 = \"2017-05-04 10:20:00\"; String date2 = \"2017-05-04 10:30:00\"; Date startDate = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(date1); Date endDate = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(date2); boolFilter.must(FilterBuilders.rangeFilter(\"createDate\").from(startDate.getTime())); boolFilter.must(FilterBuilders.rangeFilter(\"createDate\").to(endDate.getTime())); SearchResponse searchResponse = srb.setPostFilter(boolFilter) //pageNo .setFrom(pageNo&gt;0?((pageNo-1)*pageSize):0) //pageSize .setSize(pageSize) //按 id 倒序 .addSort(\"id\", SortOrder.DESC) .execute().actionGet(); SearchHits hits = searchResponse.getHits(); //总记录数 long total = hits.getTotalHits(); System.out.println(\"总记录数：\" + total); SearchHit[] searchHits = hits.hits(); List&lt;Test&gt; list = new ArrayList&lt;Test&gt;(); for (int i = 0; i &lt; searchHits.length; i++) &#123; Map&lt;String,Object&gt; map= searchHits[i].getSource(); Test t = this.recoreEntity(map); list.add(t); &#125; return list; &#125; /** * 查询结果映射为对应的实体 */ public Test recoreEntity(Map&lt;String,Object&gt; map)&#123; Calendar calendar = Calendar.getInstance(); Test test = new Test(); for (String key : map.keySet()) &#123; if(\"id\".equals(key))&#123; test.setId(Long.valueOf(map.get(key).toString())); &#125;else if(\"name\".equals(key))&#123; if(map.get(key) == null)&#123; test.setName(null); &#125;else&#123; test.setName(map.get(key).toString()); &#125; &#125;else if(\"age\".equals(key))&#123; test.setAge(Integer.parseInt((map.get(key).toString()))); &#125;else if(\"createDate\".equals(key))&#123; if(map.get(key) == null)&#123; test.setCreateDate(null); &#125;else &#123; calendar.setTimeInMillis(Long.parseLong(map.get(key).toString())); test.setCreateDate(calendar.getTime()); &#125; &#125; &#125; return test; &#125; TEST 查询createDate在 &quot;2017-05-04 10:20:00&quot; 和&quot;2017-05-04 10:30:00&quot;之间的数据pageNo 为1 pageSize 默认5123456789/** * 查询 createDate 在某个区间内 */ String date1 = \"2017-05-04 10:20:00\"; String date2 = \"2017-05-04 10:30:00\"; Date startDate = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(date1); Date endDate = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\").parse(date2); boolFilter.must(FilterBuilders.rangeFilter(\"createDate\").from(startDate.getTime())); boolFilter.must(FilterBuilders.rangeFilter(\"createDate\").to(endDate.getTime()));","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://imice.me/tags/elasticsearch/"}]},{"title":"利用mysql触发器创建变更表推送数据到elasticsearch","date":"2017-05-04T03:23:26.174Z","path":"2017/05/04/mysql_elsticsearch/","text":"利用mysql触发器创建变更表推送数据到elasticsearch 需求：某项业务需要用到模糊查询，数据库用的mysql，%search%类似这样的索引无法生效，导致搜索特别吃力看到了这位老铁的利用mysql触发器创建变更表推送数据到elasticsearch原文链接：http://www.toozy.cn/mysql-trigger-elasticsearch/ 按照这个思路，决定采用elasticsearch来替代mysql提供搜索服务。 搜索了挺多mysql和elasticsearch之间数据同步的方式，这是常见的几款插件，大神总结了这几款插件的优缺点 any way,这几款插件对我帮助不大，我只需要这一块儿业务即可，有想研究的可以查看原文http://blog.csdn.net/laoyang360/article/details/51771621于是便有了如下的解决方案： 将要搜索的数据填充到elasticsearch里面并保持更新，原文列出了几种解决方案 在应用层面当出现增删改时顺便或提交到队列来更新索引。 解析mysql的binlog，来更新索引。 利用mysql的触发器来将变更信息写入一个表或外部队列来更新索引。 （1）出于不增加业务复杂度的考虑，放弃了在应用处理的方案。（2）第二种binblog方式，原文说是出于运维部署，以及对于binlog方式如何处理单表的顾虑，放弃了binlog方案，这货不是今天的主题先略过，回头再研究…（3）最后采取的是利用mysql触发器来将变更插入一张表，通过计划任务处理变更表里的数据变更表方案给要操作的表增加增删改的触发器，将变更表id写入一张change表，服务器实时读取变更表数据，同步更新elsearch对应索引。 创建增删改的触发器 一张测试表test用来模拟我们要操作的表， 一张用来保存test表记录变更的test_change表， change_type用来保存test表触发的类型，(增删改)change_id用来保存更新字段的idlaster_id 持久化一个最后更新的id标识接下来创建三个简单的触发器123456789#增加CREATE TRIGGER test_trigger_insert AFTER INSERT ON test FOR EACH ROW INSERT INTO test_change VALUES(NULL,now(),'INSERT',NEW.id,null);#修改CREATE TRIGGER test_trigger_update AFTER UPDATE ON testFOR EACH ROW INSERT INTO test_change VALUES(NULL,now(),'UPDATE',NEW.id,null);#删除CREATE TRIGGER test_trigger_delete AFTER DELETE ON testFOR EACH ROW INSERT INTO test_change VALUES(NULL,now(),'DELETE',OLD.id,null); 触发器正常的工作，记录下了变更表的操作记录 elasticsearch安装 当时装的过程中遇到不少坑… elasticsearch下载。下载完成后是一个压缩包，在需要安装的目录解压 配置环境变量path，指向elsearch目录下的bin jdk安装。。。 增加环境变量ES_HEAP_SIZE，值为：256m（或者512m）。这是分配最大和最小的存储容量。 找到bin文件夹下的elasticsearch.bat，双击运行。在bin的同级目录就会生成data和logs文件夹。执行后可以看到类似如下界面： 验证是否安装成功，在浏览器地址栏上输入：http://localhost:9200显示一串json数据，显示你的elsearch的信息12345678910111213&#123; \"status\" : 200, \"name\" : \"Arabian Knight\", \"cluster_name\" : \"elasticsearch\", \"version\" : &#123; \"number\" : \"1.4.4\", \"build_hash\" : \"c88f77ffc81301dfa9dfd81ca2232f09588bd512\", \"build_timestamp\" : \"2015-02-19T13:05:36Z\", \"build_snapshot\" : false, \"lucene_version\" : \"4.10.3\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; elasticsearch插件安装 我只装了一个可视化的head插件，有其他需求可以自行安装插件安装命令：D:\\elasticsearch\\bin&gt;plugin -install mobz/elasticsearch-head安装完成后在D:\\elasticsearch\\plugins目录下会有head的文件夹打开head_site\\index.html(或者在浏览器地址栏输入http://localhost:9200/_plugin/head/),会显示如下界面： 服务器实时读取变更数据，并写入elasticsearch 获取elsearch实例12345678910&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;1.4.4&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.1.3&lt;/version&gt; &lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142package com.bing.utils;import org.elasticsearch.client.transport.TransportClient;import org.elasticsearch.common.settings.ImmutableSettings;import org.elasticsearch.common.settings.Settings;import org.elasticsearch.common.transport.InetSocketTransportAddress;import java.lang.reflect.Constructor;import java.util.HashMap;import java.util.Map;/** * Created by ice on 2016/11/7. */public class EsUtil &#123; static Map&lt;String, String&gt; m = new HashMap&lt;String, String&gt;(); // 设置client.transport.sniff为true来使客户端去嗅探整个集群的状态，把集群中其它机器的ip地址加到客户端中， static Settings settings = ImmutableSettings.settingsBuilder().put(m)// .put(\"cluster.name\",\"elasticsearch\")// .put(\"client.transport.sniff\", true) .put(\"index.refresh_interval\",\"1s\")//索引自动刷新间隔时间 .build(); // 创建私有对象 private static TransportClient client; static &#123; try &#123; Class&lt;?&gt; clazz = Class.forName(TransportClient.class.getName()); Constructor&lt;?&gt; constructor = clazz.getDeclaredConstructor(Settings.class); constructor.setAccessible(true); client = (TransportClient) constructor.newInstance(settings); client.addTransportAddress(new InetSocketTransportAddress(\"127.0.0.1\", 9300)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; //取得实例 public static TransportClient getTransportClient() &#123; return client; &#125;&#125; 原有数据初始化 导入当前的数据至elsearch中，作为初始数据12345678910111213141516171819202122232425262728293031323334353637/** * 初始化数据 */ @Override public void init() &#123; /** * 设置字段类型 * 为什么需要设置字段类型？ * elasticSearch对字符串有两种完全不同的搜索方式， * （1）按照整个文本进行匹配，即我们平时用的关键词搜索(keyword search) * （2）按单个字符匹配，即全文搜索(full-text search) * 对ElasticSearch稍有了解的人都知道, 前者的字符串被称为 not-analyzed 字符, 而后者被称作 analyzed 字符串. * 我的需求只是简单的关键词搜索，用不到分词啥的所以要设置字段类型为String * 注：在新的elaeticsearch5.0中，String类型已被移除，增加了两个类型 * keyword 类型对应关键词搜索 ， text 类型对应全文搜索 */ //创建索引 EsUtil.getTransportClient().admin().indices().prepareCreate(\"test\").execute(); try &#123; XContentBuilder builder = XContentFactory.jsonBuilder().startObject().startObject(\"properties\") .startObject(\"id\").field(\"type\",\"long\").endObject() .startObject(\"name\").field(\"type\",\"string\").field(\"index\",\"not_analyzed\").endObject() .startObject(\"createDate\").field(\"type\",\"string\").field(\"index\",\"not_analyzed\").endObject() .startObject(\"age\").field(\"type\",\"long\").endObject() .endObject().endObject(); PutMappingRequest putMappingRequest = Requests.putMappingRequest(\"test\").type(\"testInfo\").source(builder); EsUtil.getTransportClient().admin().indices().putMapping(putMappingRequest).actionGet(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; //初始化索引数据 List&lt;Test&gt; testList = testRepository.findAll(); for (Test test : testList) &#123; this.add(String.valueOf(test.getId())); &#125; &#125; 数据初始化之后打开我们的head视图http://localhost:9200/_plugin/head/可以看到我们的索引创建成功，浏览一下数据 向elsearch中插入数据123456789101112131415/** * 新增索引数据 * @param testId */ @Override public void add(String testId) &#123; Test test = testRepository.findOne(Long.valueOf(testId)); Map&lt;String,Object&gt; source = new HashMap&lt;String,Object&gt;(); source.put(\"id\",test.getId()); source.put(\"name\",test.getName()); source.put(\"createDate\",test.getCreateDate().getTime()); source.put(\"age\",test.getAge()); EsUtil.getTransportClient().prepareIndex(\"test\",\"testInfo\",testId) .setSource(source).execute().actionGet(); &#125;&gt;保存日期数据时，我保存的是毫秒数，方便后续利用elsearch的javaAPI根据日期进行查询&gt;（因为es默认存储时间的格式是UTC时间，如果我们查询es然后获取时间日期默认的数据，会发现跟当前的时间差8个小时，格式也是形如这样的1970-01-01T00:00:00Z，当然也可以直接设置format为你想要的格式，比如yyyy-MM-dd HH:mm:ss 然后存储的时候，指定格式，并且 Mapping 也是指定相同的format）从elsearch中删除数据12345678/** * 删除索引数据 * @param testId */ @Override public void del(String testId) &#123; EsUtil.getTransportClient().prepareDelete(\"test\",\"testInfo\", testId).execute().actionGet(); &#125;轮询数据库 相当于实时监听数据库，关于这个有很多方案，我这里的搜索功能对即时要求不太高，就用简单的轮询，时间间隔设置不要太短就行，以免服务器压力过大比如说我用spring的@Scheduled1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.bing.controller;import com.bing.domain.student.TestChange;import com.bing.repository.TestChangeRepository;import com.bing.repository.TestRepository;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;/** * Created by ice on 2017/4/28. */@Controller@RequestMapping(\"/test\")public class TestChangeController &#123; @Autowired TestChangeRepository testChangeRepository; @Autowired TestRepository testRepository; @Scheduled(cron = \"0/2 * * * * ?\") public void task()&#123; Integer n =0; TestChange tc = null; if(testChangeRepository.findAll().size() &gt; 0)&#123; tc = testChangeRepository.findAll().get(0); &#125; //最后更新id标识 Integer count = testChangeRepository.getCountAsSql(); if(tc != null)&#123; n = tc.getLasterId(); &#125; count = count==null?0:count; n = n==null?0:n; if(count &gt; n )&#123; //循环的次数count-n for (int i = (n+1);i&lt;=count; i++) &#123; TestChange testChange = testChangeRepository.findOne(Long.valueOf(i)); Long changeId = Long.valueOf(testChange.getChangeId()); String chanType = testChange.getChangeType(); if(\"DELETE\".equals(chanType))&#123; //根据id从elasticsearch中删除对应数据 testChangeRepository.del(String.valueOf(changeId)); &#125;else if(\"INSERT\".equals(chanType))&#123; //向elsearch中添加数据 testChangeRepository.add(String.valueOf(changeId)); &#125;else if(\"UPDATE\".equals(chanType))&#123; //因为我们不知道要更改的属性， // 数据更新的时候只有把elsearch中原本对应的数据删除，再添加新的数据 testChangeRepository.del(String.valueOf(changeId)); testChangeRepository.add(String.valueOf(changeId)); &#125; &#125; &#125; n = count; if(tc != null)&#123; tc.setLasterId(n); testChangeRepository.save(tc); &#125; &#125; @RequestMapping(\"/init\") public void init()&#123; testChangeRepository.init(); &#125;&#125; 测试 基本配置已经完成，可以手动更改数据库中的数据来测试是否会同步到elasticsearch 接下来就是调用elsearch的javaapi来做数据查询，恩就放到下一篇吧。。。","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://imice.me/tags/elasticsearch/"},{"name":"mysql","slug":"mysql","permalink":"http://imice.me/tags/mysql/"}]},{"title":"手机端页面下滑异步加载数据","date":"2017-04-27T02:27:32.310Z","path":"2017/04/27/slideLoading/","text":"手机端页面下滑异步加载数据 需求：手机端滑动异步加载产品数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//当前pageNovar pageNo = parseInt('$&#123;pageNo&#125;');$(function()&#123; query('first');//首次加载&#125;);//设置加载状态var loading = false;Zepto(function($)&#123; $(window).scroll(function () &#123; //默认滑动到底部开始 if(($(window).scrollTop() + $(window).height() &gt; $(document).height()-10) &amp;&amp; loading )&#123; loading=false; pageNo++; query(\"then\"); &#125; &#125;);&#125;);function query(type)&#123; var url = \"$&#123;ctx&#125;/product/getJson.action?pageNo=\"+pageNo; $loading.show();//显示加载中效果 $.ajax(&#123; url:url, success:function()&#123; loading = true;//标识 if(data == null)&#123; pageNo--; $.loading.hide(); $('.nomore-products').show();//数据加载完成，无更多 loading = false; &#125;else&#123; if(type == 'then')&#123;//滑动加载 if(data.length == 0)&#123; pageNo--; $loading.hide(); $('.nomore-products').show(); loading = false; return \"\"; &#125; &#125; $.each(data,function(i,item)&#123; //获取json数据拼接html $productList.append( '&lt;figcaption&gt;' +'&lt;span&gt;' + item.productTitle + '&lt;/span&gt;' +'&lt;span&gt;' + item.productId+ '&lt;/span&gt;' +'&lt;/figcaption&gt;' ); $loading.hide(); &#125;); &#125; &#125;, error:function(XMLHttpRequest,textStatus,errorThrown)&#123; loading=true; pageNo--; alert(\"数据加载出错\"+textStatus); &#125; &#125;); 如果首次加载不想用异步方式，可以在pageNo声明之后直接++，直接加载第二页，首次可以用从后台直接返回的list数据，异步加载时再返回json即可","tags":[{"name":"滑动异步加载","slug":"滑动异步加载","permalink":"http://imice.me/tags/滑动异步加载/"}]},{"title":"正则表达式匹配url","date":"2017-04-13T10:16:31.934Z","path":"2017/04/13/zhengze/","text":"正则表达式匹配url 需求：在客户端识别一段文字中的url并加上链接效果 12345678&lt;script type=\"text/javascript\"&gt; function replaceReg(reg,str)&#123; return str.replace(reg,function(m)&#123;return '&lt;a href=\"http://'+m+'\" target=\"_blank\"&gt;'+m+'&lt;/a&gt;';&#125;) &#125; var reg = /[\\w\\-]+(\\.[\\w\\-]+)+([\\w\\-\\.,@?^=%&amp;:\\/~\\+#]*[\\w\\-\\@?^=%&amp;\\/~\\+#])?/ig; var str ='普通ice1025.github.io前缀http://ice1025.github.io带参数https://ice1025.github.io/2017/04/05/categories_%20java/复杂参数https://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=1&amp;tn=baidu&amp;wd=test&amp;oq=ice1025.github.io&amp;rsv_pq=aff940260000ac5b&amp;rsv_t=7296ebK9LCPPnnupMO6dFV7jFM9hTjiVM24P3ETj4%2BH%2F3W%2BPlK0wix6RqS4&amp;rqlang=cn&amp;rsv_enter=1&amp;rsv_sug3=12&amp;rsv_sug1=12&amp;rsv_sug7=100&amp;rsv_sug2=0&amp;inputT=4695&amp;rsv_sug4=4695'; document.write(replaceReg(reg,str));&lt;/script&gt; 可以看到这里正则开头并没有限制以https | http | ftp开头，而是在输出时添加http://原因： 通过正则中 | + space确实可以匹配到类似www.xx.com这种网址但是我发现点击跳转的时候 target 失效，形成 localhost/www.xx.com 这种网址 页面效果如图： 头部选择不匹配，如有需要可以在头部加上((ht|f)tps?):\\/\\/","tags":[{"name":"正则表达式","slug":"正则表达式","permalink":"http://imice.me/tags/正则表达式/"}]}]